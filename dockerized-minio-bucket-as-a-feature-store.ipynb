{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Building a Feature Store with a MinIO Bucket and the Docker SDK\n##### By: David Cannan Cdaprod\n\nTo achieve secure and containerized execution of scripts while logging the results in a stateful manner using Docker, here's a detailed approach. We will use the Docker Python SDK to manage the Docker containers and interact with MinIO to fetch the scripts.\n\n### Prerequisites\n1. Ensure the Docker daemon is accessible from your iPhone using the Juno iOS Python notebooks.\n2. Ensure you have the `docker` and `minio` Python packages installed:\n    ```bash\n    pip install docker minio\n    ```\n\n### 1. Create a Dockerfile for Your Script\nCreate a `Dockerfile` that defines the environment for running your scripts. For example:\n```Dockerfile\n# Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory in the container\nWORKDIR /usr/src/app\n\n# Copy the script into the container\nCOPY your_script.sh /usr/src/app/your_script.sh\n\n# Make the script executable\nRUN chmod +x /usr/src/app/your_script.sh\n\n# Define the command to run the script\nCMD [\"./your_script.sh\"]\n```\n\n### 2. Write the Python Script to Fetch and Execute Scripts from MinIO\nBelow is the Python script to fetch the script from MinIO, build a Docker image, run the container, and log the output.\n\n#### Python Script (`run_script.py`):"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To ensure the script can take a URL as an input parameter when running, you can use command-line arguments. The `argparse` library in Python is perfect for this purpose. Here‚Äôs how you can modify the script to accept the URL as an input parameter:\n\n### Updated Web Scraping Script with Command-Line Arguments\n\n```python\nimport os\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_web_page(url):\n    \"\"\"\n    Scrapes the specified web page and returns the text content.\n\n    Args:\n        url (str): The URL of the web page to scrape.\n\n    Returns:\n        str: The text content of the web page.\n    \"\"\"\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            soup = BeautifulSoup(response.content, 'html.parser')\n            page_text = soup.get_text(separator='\\n')\n            return page_text\n        else:\n            return f\"Failed to retrieve the page. Status code: {response.status_code}\"\n    except requests.RequestException as e:\n        return f\"An error occurred while making the request: {str(e)}\"\n\nif __name__ == \"__main__\":\n    url = os.environ.get(\"URL\")\n    if url:\n        result = scrape_web_page(url)\n        print(result)\n    else:\n        print(\"Error: No URL provided.\")\n```\n\n### Example Usage\n\nYou can run the script from the command line, providing the function call as a JSON string:\n\n```bash\npython3 llm_web_scraper.py '{\"function_name\": \"scrape_web_page\", \"inputs\": {\"url\": \"https://example.com\"}}'\n```\n\n### Explanation\n\n1. **scrape_web_page Function**:\n   - Scrapes the specified web page and returns the text content.\n   - Handles HTTP requests and parses the HTML content using BeautifulSoup.\n\n2. **run_llm_function**:\n   - Takes a function call string in JSON format as input.\n   - Parses the string to extract the function name and inputs.\n   - Calls the specified function with the provided inputs.\n   - Handles errors and returns appropriate messages.\n\n3. **main Function**:\n   - Uses the `argparse` library to handle command-line arguments.\n   - Accepts the function call as a JSON string from the command line.\n   - Calls the `run_llm_function` with the provided input and prints the result.\n\n### Running the Script\n\nSave the script to a file, for example, `llm_web_scraper.py`. You can then run it from the command line, passing the function call as a JSON string as shown in the example usage.\n\nThis setup allows you to dynamically set the URL (or any other inputs) when running the script, making it flexible and easy to integrate with an AI or other systems."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n### Steps to Execute the Script\n1. Ensure your Docker daemon is set up to allow remote connections.\n2. Save the below Python script (`run_feature.py`) on your iPhone within Juno iOS Python notebooks.\n3. Run the Python script to fetch the script from MinIO, build the Docker image, and execute the script in a container.\n\nThis approach ensures that your script runs in a secure and isolated environment (Docker container) and logs the output in a stateful manner. The logs are saved to a file on the host machine, ensuring you can review the output after execution."
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true
   },
   "cell_type": "code",
   "source": "# run_feature.py\nimport docker\nfrom minio import Minio\nfrom minio.error import S3Error\nimport os\nimport uuid\nimport datetime\nimport re\n\nclass FeatureStoreInitializer:\n    def __init__(self, minio_config, docker_config):\n        self.minio_client = Minio(**minio_config)\n        self.docker_client = docker.DockerClient(**docker_config)\n        self.bucket_name = \"function-bucket\"\n    \n    def create_bucket(self):\n        try:\n            if not self.minio_client.bucket_exists(self.bucket_name):\n                self.minio_client.make_bucket(self.bucket_name, location=\"us-east\")\n                print(f\"Bucket {self.bucket_name} created successfully.\")\n            else:\n                print(f\"Bucket {self.bucket_name} already exists.\")\n        except S3Error as exc:\n            if exc.code in ['BucketAlreadyOwnedByYou', 'BucketAlreadyExists']:\n                print(f\"Bucket {self.bucket_name} already exists.\")\n            else:\n                print(f\"Error occurred: {exc}\")\n\n    def fetch_script(self, object_name, file_path):\n        try:\n            self.minio_client.fget_object(self.bucket_name, object_name, file_path)\n            print(f\"Script {object_name} fetched and saved to {file_path}\")\n        except S3Error as exc:\n            print(f\"Error occurred: {exc}\")\n\n    def build_docker_image(self, script_path, dockerfile_path='Dockerfile', image_name='script_executor'):\n        if not os.path.exists(dockerfile_path):\n            dockerfile_content = f\"\"\"\nFROM python:3.9-slim\nWORKDIR /usr/src/app\nCOPY {os.path.basename(script_path)} /usr/src/app/{os.path.basename(script_path)}\nRUN pip install requests beautifulsoup4\nRUN chmod +x /usr/src/app/{os.path.basename(script_path)}\nCMD [\"python\", \"/usr/src/app/{os.path.basename(script_path)}\"]\n\"\"\"\n            with open(dockerfile_path, 'w') as dockerfile:\n                dockerfile.write(dockerfile_content.strip())\n            print(f\"Generated default Dockerfile at {dockerfile_path}\")\n        else:\n            print(f\"Using existing Dockerfile at {dockerfile_path}\")\n\n        image, logs = self.docker_client.images.build(path='.', tag=image_name, dockerfile=dockerfile_path, nocache=True)\n        for log in logs:\n            print(log)\n        return image\n\n    def clean_up_old_containers(self):\n        for container in self.docker_client.containers.list(all=True):\n            if 'script_container_' in container.name:\n                container.remove(force=True)\n                print(f\"Removed old container: {container.name}\")\n\n    def clean_url(self, url):\n        cleaned_url = re.sub(r'https?://', '', url)  # Remove http:// or https://\n        cleaned_url = cleaned_url.replace('.com', '')  # Remove .com\n        return re.sub(r'\\W+', '_', cleaned_url)\n\n    def run_docker_container(self, image_name, environment):\n        container_name = f'script_container_{uuid.uuid4()}'\n        container = self.docker_client.containers.run(image_name, name=container_name, detach=True, environment=environment)\n        container_id = container.id[:3]\n        \n        cleaned_url = self.clean_url(environment[\"URL\"])\n        log_file_name = f'scrape_{cleaned_url}_{container_id}_logs.txt'\n        \n        logs = container.logs(stream=True)\n        \n        log_file_path = os.path.join(os.getcwd(), log_file_name)\n        with open(log_file_path, 'w') as log_file:\n            for log in logs:\n                log_line = log.decode('utf-8').strip()\n                if log_line:\n                    log_file.write(f\"{log_line}\\n\")\n                    print(log_line)\n        \n        container.wait()\n        container.remove()\n        print(f\"Logs saved to {log_file_path}\")\n\n    def initialize(self, script_name, environment):\n        script_path = os.path.join(os.getcwd(), script_name)\n        self.create_bucket()\n        self.fetch_script(script_name, script_path)\n        self.build_docker_image(script_path)\n        self.clean_up_old_containers()\n        self.run_docker_container('script_executor', environment)\n\ndef scrape_url(url):\n    minio_config = {\n        \"endpoint\": \"192.168.0.25:9000\",\n        \"access_key\": \"cda_cdaprod\",\n        \"secret_key\": \"cda_cdaprod\",\n        \"secure\": False,\n        \"region\": \"us-east\"\n    }\n\n    docker_config = {\n        \"base_url\": 'tcp://rpi4-2.local:2375'\n    }\n\n    initializer = FeatureStoreInitializer(minio_config, docker_config)\n    initializer.initialize(\"scrape_web_page.py\", {\"URL\": url})\n\n# Example usage within the notebook\nscrape_url(\"https://github.com/cdaprod/cdaprod\")  # Replace this with the actual URL you want to scrape",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Bucket function-bucket already exists.\nScript scrape_web_page.py fetched and saved to /private/var/mobile/Library/Mobile Documents/iCloud~com~rationalmatter~junoapp/Documents/My Scripts/docker-sdk/minio-fs/scrape_web_page.py\nUsing existing Dockerfile at Dockerfile\n{'stream': 'Step 1/6 : FROM python:3.9-slim'}\n{'stream': '\\n'}\n{'stream': ' ---> 0473f6c1be93\\n'}\n{'stream': 'Step 2/6 : WORKDIR /usr/src/app'}\n{'stream': '\\n'}\n{'stream': ' ---> Running in 805243454de8\\n'}\n{'stream': ' ---> ccc510ba5294\\n'}\n{'stream': 'Step 3/6 : COPY scrape_web_page.py /usr/src/app/scrape_web_page.py'}\n{'stream': '\\n'}\n{'stream': ' ---> d1ce8999a6ee\\n'}\n{'stream': 'Step 4/6 : RUN pip install requests beautifulsoup4'}\n{'stream': '\\n'}\n{'stream': ' ---> Running in d0bbac47d0d1\\n'}\n{'stream': 'Collecting requests\\n'}\n{'stream': '  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\\n'}\n{'stream': '     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 64.9/64.9 kB 1.1 MB/s eta 0:00:00'}\n{'stream': '\\n'}\n{'stream': 'Collecting beautifulsoup4\\n'}\n{'stream': '  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\\n'}\n{'stream': '     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 147.9/147.9 kB 3.6 MB/s eta 0:00:00'}\n{'stream': '\\n'}\n{'stream': 'Collecting certifi>=2017.4.17\\n'}\n{'stream': '  Downloading certifi-2024.6.2-py3-none-any.whl (164 kB)\\n'}\n{'stream': '     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 164.4/164.4 kB 4.3 MB/s eta 0:00:00'}\n{'stream': '\\n'}\n{'stream': 'Collecting idna<4,>=2.5\\n'}\n{'stream': '  Downloading idna-3.7-py3-none-any.whl (66 kB)\\n'}\n{'stream': '     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 66.8/66.8 kB 3.1 MB/s eta 0:00:00'}\n{'stream': '\\n'}\n{'stream': 'Collecting charset-normalizer<4,>=2\\n'}\n{'stream': '  Downloading charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (138 kB)\\n'}\n{'stream': '     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 138.3/138.3 kB 1.5 MB/s eta 0:00:00'}\n{'stream': '\\n'}\n{'stream': 'Collecting urllib3<3,>=1.21.1\\n'}\n{'stream': '  Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\\n'}\n{'stream': '     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 121.4/121.4 kB 3.8 MB/s eta 0:00:00'}\n{'stream': '\\n'}\n{'stream': 'Collecting soupsieve>1.2\\n'}\n{'stream': '  Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\\n'}\n{'stream': 'Installing collected packages: urllib3, soupsieve, idna, charset-normalizer, certifi, requests, beautifulsoup4\\n'}\n{'stream': 'Successfully installed beautifulsoup4-4.12.3 certifi-2024.6.2 charset-normalizer-3.3.2 idna-3.7 requests-2.32.3 soupsieve-2.5 urllib3-2.2.2\\n'}\n{'stream': \"\\x1b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\\n\\x1b[0m\"}\n{'stream': '\\x1b[91m\\n[notice] A new release of pip is available: 23.0.1 -> 24.1.1\\n[notice] To update, run: pip install --upgrade pip\\n\\x1b[0m'}\n{'stream': ' ---> ab9e93260ff4\\n'}\n{'stream': 'Step 5/6 : RUN chmod +x /usr/src/app/scrape_web_page.py'}\n{'stream': '\\n'}\n{'stream': ' ---> Running in ad31ee6de84f\\n'}\n{'stream': ' ---> ebe4776531f4\\n'}\n{'stream': 'Step 6/6 : CMD [\"python\", \"/usr/src/app/scrape_web_page.py\"]'}\n{'stream': '\\n'}\n{'stream': ' ---> Running in 42b610ae4834\\n'}\n{'stream': ' ---> d291dbb53e16\\n'}\n{'aux': {'ID': 'sha256:d291dbb53e16ba28ae3b06d35c928c2f9194cb54415951ff01ed7ddfc6177478'}}\n{'stream': 'Successfully built d291dbb53e16\\n'}\n{'stream': 'Successfully tagged script_executor:latest\\n'}\nGitHub - Cdaprod/Cdaprod: Open-Source Contributor\nSkip to content\nNavigation Menu\nToggle navigation\nSign in\nProduct\nActions\nAutomate any workflow\nPackages\nHost and manage packages\nSecurity\nFind and fix vulnerabilities\nCodespaces\nInstant dev environments\nGitHub Copilot\nWrite better code with AI\nCode review\nManage code changes\nIssues\nPlan and track work\nDiscussions\nCollaborate outside of code\nExplore\nAll features\nDocumentation\nGitHub Skills\nBlog\nSolutions\nBy size\nEnterprise\nTeams\nStartups\nBy industry\nHealthcare\nFinancial services\nManufacturing\nBy use case\nCI/CD & Automation\nDevOps\nDevSecOps\nResources\nResources\nLearning Pathways\nWhite papers, Ebooks, Webinars\nCustomer Stories\nPartners\nOpen Source\nGitHub Sponsors\nFund open source developers\nThe ReadME Project\nGitHub community articles\nRepositories\nTopics\nTrending\nCollections\nEnterprise\nEnterprise platform\nAI-powered developer platform\nAvailable add-ons\nAdvanced Security\nEnterprise-grade security features\nGitHub Copilot\nEnterprise-grade AI features\nPremium Support\nEnterprise-grade 24/7 support\nPricing\nSearch or jump to...\nSearch code, repositories, users, issues, pull requests...\nSearch\nClear\nSearch syntax tips\nProvide feedback\nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancel\nSubmit feedback\nSaved searches\nUse saved searches to filter your results more quickly\nName\nQuery\nTo see all available qualifiers, see our\ndocumentation\n.\nCancel\nCreate saved search\nSign in\nSign up\nYou signed in with another tab or window.\nReload\nto refresh your session.\nYou signed out in another tab or window.\nReload\nto refresh your session.\nYou switched accounts on another tab or window.\nReload\nto refresh your session.\nDismiss alert\nCdaprod\n/\nCdaprod\nPublic\nNotifications\nYou must be signed in to change notification settings\nFork\n0\nStar\n2\nOpen-Source Contributor\nsanity.cdaprod.dev\n2\nstars\n0\nforks\nBranches\nTags\nActivity\nStar\nNotifications\nYou must be signed in to change notification settings\nCode\nPull requests\n0\nDiscussions\nActions\nProjects\n0\nWiki\nSecurity\nInsights\nAdditional navigation options\nCode\nPull requests\nDiscussions\nActions\nProjects\nWiki\nSecurity\nInsights\nCdaprod/Cdaprod\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\nmain\nBranches\nTags\nGo to file\nCode\nFolders and files\nName\nName\nLast commit message\nLast commit date\nLatest commit\nHistory\n60 Commits\n.github/\nworkflows\n.github/\nworkflows\npublic\npublic\nresumes @ cc9beb0\nresumes @ cc9beb0\n.gitmodules\n.gitmodules\nAntisyphon Certificate of Completion.pdf\nAntisyphon Certificate of Completion.pdf\nB419057C-FD73-4687-808C-F3719C7CA4BD.png\nB419057C-FD73-4687-808C-F3719C7CA4BD.png\nBIZ-PLAN.md\nBIZ-PLAN.md\nCdaprod - June 1, 2023 21.20.58.png\nCdaprod - June 1, 2023 21.20.58.png\nFUNDING.yml\nFUNDING.yml\nProfessional_Security_Statement.md\nProfessional_Security_Statement.md\nREADME-3.md\nREADME-3.md\nREADME.md\nREADME.md\nSMART_Goals.html\nSMART_Goals.html\nSMART_Goals_Markdown.md\nSMART_Goals_Markdown.md\nmy-data.json\nmy-data.json\nrepos.md\nrepos.md\ntext.txt\ntext.txt\nView all files\nRepository files navigation\nREADME\nDavid's Developer Journey\nMy MinIO Publications List\nMy Resumes Repo\nAbout Me\nAs a MinIO DevOps and an aspiring tech innovator, I'm deeply engaged in building personal AI solutions using the OpenAI API, Python LangChain Framework, and Mistral LLM via LM Studios. My journey into the tech world is self-taught, rooted in a passion for hacking, software engineering, DevOps, programming, cloud technologies, containerization, Infrastructure as Code (IaC), Software as a Service (SaaS), cybersecurity, and bug bounty hunting. Overcoming addiction, I've reshaped my life to focus on being a dedicated single father to my triplet boys, embodying resilience and determination.\nCurrently, I'm honing my skills as a DevOps Intern at MinIO, where I'm writing\narticles\nand gaining invaluable hands-on experience with cutting-edge cloud storage technologies. My ambition is to establish a strong personal brand as a self-made success in the tech field. I'm particularly interested in projects that offer financial potential, including software development for web apps, digital downloads, custom bots, and data lake applications with machine learning capabilities.\nMy Approach and Methodology\nMy approach to projects is to seek streamlined, structured development guidance for app modules, aiming for efficiency and effectiveness in my work. My social media presence, including this GitHub,\nTwitter\n,\nLinkedIn\n, and my\nblog\n, reflects my journey and interests. Living in Douglasville, Georgia, USA, I'm an advocate for sharing my experiences and insights with the tech community, using hashtags like #devopsdad, #codenewbie, #tripletdad, and #hacktheplanet to connect with like-minded individuals.\nMy methodology emphasizes being\nextra\nand overly technical, breaking down tasks into subtasks, prioritizing them, and executing them in sequences. This structured approach helps me tackle complex problems systematically. In coding requests, I prefer detailed, logical responses with necessary imports and logic, and I'm open to using tools like Mermaid.js for class diagrams.\nTech Stack and Tools\nMy tech stack includes:\nCI/CD\n: GitHub Actions (Cdaprod/)\nContainerization\n: Docker Registry (hub.docker Cdaprod/), Container Registry (Ghcr.io Cdaprod/)\nAPIs\n: LangChain API (LCEL) for LLMs, tools, and classes for API and data integration\nLanguages\n: Mastery in Python, JavaScript, Go, and Bash\nDevOps Tools\n: GitHub Actions, Docker, Kubernetes, Terraform\nCloud Platforms\n: AWS, GCP, Azure\nCyber Security\n: Bug bounty hunting, digital environment security\nWith a goal to generate passive income, I'm keen on outlining key development steps for quick project setup and execution, always looking for ways to leverage my skills for financial and professional growth.\nüöÄ Current Focus\nI am deeply engrossed in:\nAdvancing MinIO's cloud storage capabilities\n: Focusing on efficiency and scalability.\nCreating innovative personal AI tools\n: Leveraging the OpenAI API for diverse applications.\nEnhancing development workflows\n: Through containerization, Infrastructure as Code (IaC), and robust CI/CD pipelines to optimize efficiency and reliability.\nExploring Electrical Engineering & IoT\n: Designing and developing smart systems integrating IoT devices with hybrid cloud solutions for automation and control.\nAI Automation\n: Implementing AI-driven processes to enhance efficiency, accuracy, and adaptability across various applications.\nResearch and Exploratory Development\n: Pursuing innovative ideas, conducting experiments, and exploring new technologies to stay at the cutting edge of tech advancements.\nüí° Projects\nMy GitHub repositories showcase my ventures in:\nWeb Applications\n: Crafting dynamic, intuitive interfaces with Python and JavaScript, focusing on user engagement and functionality.\nDigital Downloads and Custom Bots\n: Developing automated solutions and bots that streamline tasks, enhance productivity, and offer unique digital experiences.\nData Lake Applications with ML\n: Utilizing machine learning to delve into vast datasets, extracting valuable insights and facilitating data-driven decision-making.\nElectrical Engineering & IoT\n: Designing and developing smart systems integrating IoT devices with hybrid cloud solutions for automation and control.\nAI Automation\n: Implementing AI-driven processes to enhance efficiency, accuracy, and adaptability across various applications.\nResearch and Exploratory Development\n: Pursuing innovative ideas, conducting experiments, and exploring new technologies to stay at the cutting edge of tech advancements.\nüõ† Skills & Technologies\nMy technical toolkit includes:\nLanguages\n: Mastery in Python, JavaScript, Go, and Bash, enabling versatile software development.\nDevOps Tools\n: Proficient with GitHub Actions, Docker, Kubernetes, and Terraform for seamless development and deployment.\nCloud Platforms\n: Skilled in deploying and managing applications on AWS, GCP, and Azure, ensuring high availability and performance.\nCyber Security & Bug Bounty Hunting\n: Dedicated to securing digital environments, applying rigorous methodologies to identify and mitigate vulnerabilities.\nElectrical Engineering\n: Expertise in circuit design, PCB layout, and integrating hardware with software for IoT and automation projects.\nüå± Learning & Growth\nMy journey of exploration and enhancement focuses on:\nAdvanced AI & ML Techniques\n: Pursuing deeper knowledge in AI and ML to build more intelligent and adaptive applications. I'm currently exploring reinforcement learning, natural language processing, and computer vision to develop cutting-edge AI solutions. Resources I use include:\nDeep Learning Specialization by Andrew Ng\nFast.ai Courses\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aur√©lien G√©ron\nCloud-Native Technologies\n: Embracing the latest in cloud-native solutions to boost the scalability and resilience of cloud services. My focus is on Kubernetes, microservices architecture, and serverless computing. Key resources include:\nKubernetes Documentation\nThe Twelve-Factor App\nAWS Lambda Getting Started\nSoftware Architecture\n: Committing to the design of robust, scalable, and maintainable software architectures that stand the test of time. I'm learning about domain-driven design, event-driven architecture, and microservices. Essential readings are:\nDomain-Driven Design by Eric Evans\nBuilding Microservices by Sam Newman\nDesigning Data-Intensive Applications by Martin Kleppmann\nIoT and Hybrid Cloud\n: Integrating IoT devices with cloud platforms to create hybrid solutions that offer enhanced capabilities and scalability. I'm currently working with MQTT, edge computing, and hybrid cloud architectures. Useful resources include:\nMQTT Essentials\nEdge Computing: A Primer\nAzure IoT Hub Documentation\nüìà Passive Income Goals\nMy aspirations for generating passive income include:\nSoftware Development\n: Innovating and developing software solutions that solve real-world problems, providing continuous value. Some of my current projects involve building SaaS applications, developing custom APIs, and creating automation tools. Resources I often reference are:\nSaaS Boilerplate\nAPI Design Patterns by JJ Geewax\nAutomate the Boring Stuff with Python by Al Sweigart\nContent Creation\n: Sharing insights, tutorials, and experiences through my\nblog\nto educate and inspire others.\n‚úçÔ∏è Selected MinIO Articles\nHere are some of my top articles on MinIO:\nBuilding and Deploying a MinIO-Powered LangChain Agent API with LangServe\nExplore the exciting possibilities of leveraging MinIO and LangChain to create a robust and efficient agent capable of handling complex data processing tasks.\nDynamic ETL Pipeline: Hydrate AI with Web Data for MinIO and Weaviate using Unstructured-IO\nThis article explores the integration of Unstructured-IO, MinIO, and Weaviate to create a dynamic ETL pipeline. This pipeline transforms unstructured web data into structured, analyzable formats, leveraging AI and metadata capabilities to unlock actionable insights.\nDisaster Proof MinIO with GitOps\nLearn how strategic automation, redundancy, and integration with Docker and GitHub ensure swift recovery from disasters, transforming potential chaos into a choreographed comeback.\nOptimizing AI Data Processing with MinIO Weaviate and Langchain in Retrieval Augmented Generation (RAG) Pipelines\nDelve into AI‚Äôs next frontier with MinIO S3 Object-Store and SDK, enhancing a Weaviate Retrieval Augmented Generation (RAG) Pipeline for robust data management. Discover how to elevate efficiency in AI systems using LangChain, unlocking new dimensions in scalable AI solutions.\nThe Future of Hybrid Cloud Pipelines: Integrating MinIO, Tailscale, and GitHub Actions\nStreamline your data processing capabilities, ensuring high-quality data management and secure operations. This integration not only enhances workflow automation but also leverages the advanced functionalities of MinIO and Tailscale, providing a powerful solution for modern data processing needs.\nDeploying Application Infrastructure with MinIO S3 and Tailscale VPN\nLearn how MinIO S3 object storage and Tailscale VPN simplify deploying secure and scalable application infrastructure. This blog covers how their integration offers secure networking, streamlined access, and advanced features to serve hosted applications with Tailscale.\nPublication Index\nHere is a complete index of my work related to MinIO. Click on the links to read the articles and dive deeper into each topic.\nNo.\nTitle\nSummary\nDate\nLink\n16\nThe Future of Hybrid Cloud Pipelines: Integrating MinIO, Tailscale, and GitHub Actions\nStreamline your data processing capabilities, ensuring high-quality data management and secure operations. This integration not only enhances workflow automation but also leverages the advanced functionalities of MinIO and Tailscale, providing a powerful solution for modern data processing needs.\n2024-05-24\nLink\n15\nDeploying Application Infrastructure with MinIO S3 and Tailscale VPN\nLearn how MinIO S3 object storage and Tailscale VPN simplify deploying secure and scalable application infrastructure. This blog covers how their integration offers secure networking, streamlined access, and advanced features to serve hosted applications with Tailscale.\n2024-05-10\nLink\n14\nOptimizing AI Data Processing with MinIO Weaviate and Langchain in Retrieval Augmented Generation (RAG) Pipelines\nDelve into AI‚Äôs next frontier with MinIO S3 Object-Store and SDK, enhancing a Weaviate Retrieval Augmented Generation (RAG) Pipeline for robust data management. Discover how to elevate efficiency in AI systems using LangChain, unlocking new dimensions in scalable AI solutions.\n2024-04-29\nLink\n13\nBuilding and Deploying a MinIO-Powered LangChain Agent API with LangServe\nExplore the exciting possibilities of leveraging MinIO and LangChain to create a robust and efficient agent capable of handling complex data processing tasks.\n2024-04-09\nLink\n",
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": "12\nMinIO Networking with Overlay Networks\nOverlay networks enable seamless multi-host deployments for MinIO‚Äôs cloud-native S3-compatible storage solutions. Emphasizing security, scalability, and robust container networking, these technologies streamline complex cloud architectures.\n2024-03-29\nLink\n11\nDisaster Proof MinIO with GitOps\nWhen disaster strikes, the power of GitOps shines, transforming potential chaos into a choreographed comeback. Learn how strategic automation, redundancy, and Docker and GitHub integration ensure swift recovery, turning system wipes into minor setbacks.\n2024-03-19\nLink\n10\nPowering AI/ML Innovation: Building Feature Stores with MinIO‚Äôs High-Performance Object Storage\nMinIO‚Äôs high-performance object storage is key for AI innovation, offering scalability and integration for feature stores. Its capabilities enable seamless ML workflows, enhancing data management for AI development and deployment, impacting sectors like e-commerce and healthcare.\n2024-03-12\nLink\n9\nDeploying MinIO with GitOps on Self-Hosted Infrastructure\nThis article unveils the synergy between MinIO and GitOps, utilizing self-hosted GitHub Actions to redefine CI/CD workflows. It emphasizes streamlined data management, security, and the efficiency of deploying scalable software.\n2024-03-05\nLink\n8\nDynamic ETL Pipeline: Hydrate AI with Web Data for MinIO and Weaviate using Unstructured-IO\nUnstructured-IO, MinIO, & Weaviate redefine ETL, turning unstructured web data into actionable insights. This collaboration enhances data management, offering a robust solution for dynamic data transformation and analysis, marking a leap in how we process and leverage web-generated content.\n2024-02-27\nLink\n7\nDeveloping Langchain Agents with the MinIO SDK for LLM Tool-Use\nExplore Langchain‚Äôs LLM Tool-Use and leverage Langgraph for monitoring MinIO‚Äôs S3 Object Store. This guide walks you through developing custom conversational AI agents and creating powerful OpenAI LLM chains for efficient data management and enhanced application functionality.\n2024-02-20\nLink\n6\nPowering AI/ML workflows with GitOps Automation\nExplore the fusion of GitOps, MinIO, Weaviate, and Python in AI development for unparalleled automation and innovation. This combination offers a solid foundation for creating scalable, efficient, and automated AI solutions, propelling projects from concept to reality with ease.\n2024-02-13\nLink\n5\nBacking Up Weaviate with MinIO S3 Buckets\nExplore integrating MinIO with Weaviate using Docker Compose for AI-enhanced data management. Learn to back up Weaviate to MinIO S3 buckets, ensuring data integrity and scalability with practical Docker and Python examples. Streamline your AI-driven search and analysis with this robust setup.\n2024-02-06\nLink\n4\nInnovating S3 Bucket Retrieval: Langchain Community S3 Loaders with OpenAI API\nExplore the synergy of MinIO, Langchain, and OpenAI in enhancing data storage and processing. This article illustrates MinIO‚Äôs integration for efficient document summarization using Langchain and OpenAI‚Äôs GPT, revolutionizing AI and ML data handling.\n2024-01-30\nLink\n3\nEvent-Driven Architecture: MinIO Event Notification Webhooks using Flask\nExplore deploying MinIO and Flask with Docker-compose for event-driven architecture. Master MinIO bucket events and Flask webhooks for efficient data workflows and robust applications. Dive into the synergy of cloud technologies.\n2024-01-23\nLink\n2\nStreamlining Data Events with MinIO and PostgreSQL\nExplore 'Streamlining Data Events with MinIO and PostgreSQL,' a guide for developers using Docker, MinIO, and PostgreSQL. Learn about using Docker Compose for real-time data events, enhancing data analytics, and developing robust, event-driven applications.\n2024-01-16\nLink\n1\nSmooth Sailing from Docker to Localhost\nExplore the integration of Dockerized MinIO with localhost Flask apps. This guide addresses Docker networking challenges, ensuring seamless MinIO and Flask communication for a development environment that closely mirrors production. Dive into practical solutions for robust workflows.\n2023-12-08\nLink\nTechnical Stack and Tools\nIn my articles, I frequently discuss and use the following tools and technologies:\nTailscale\n: Hybrid-cloud secure self hosted infrastructure\nMinIO\n: Kubernetes-native S3 object storage\nDocker\n: Containerization platform\nGitOps\n: Automation and CI/CD practices\nWeaviate\n: Vector databases and stores\nLangChain/Langgraph OpenAI\n: Leveraging AI for enhanced data processing\nContributions and Contact\nI'm always open to collaborations and contributions. If you're interested in contributing to this repository or have suggestions for topics, please feel free to\nopen an issue\nor submit a pull request.\nFor direct contact, you can reach me via:\nLinkedIn\n:\nDavid Cannan\nEmail\n:\ncdaprod@cdaprod.dev\nUpdates and Maintenance\nThis repository is updated regularly as new publications are released. Check back often for the latest articles and insights.\nAcknowledgments\nSpecial thanks to all my colleagues at MinIO and the tech community for their support and insights that have significantly influenced this work.\nü§ù Connect With Me\nI'm eager to connect, collaborate, or simply chat about technology, programming, or any topic of mutual interest:\nTwitter\n:\n@cdasmktcda\nLinkedIn\n:\nDavid Cannan\nGitHub\n:\nCdaprod\n##Here is a more extensive list of my top public and private repositories from the\ncda_POSTS\nand\ncda_REPOS\ncategories:\ncda_POSTS\ncda_POSTS/atlanta-conference-talks-2024\nRepository\n:\natlanta-conference-talks-2024\nPurpose\n: Collection of talks and presentations for the 2024 Atlanta Conference, focusing on advancements in DevOps, AI, and cloud technologies.\ncda_POSTS/developing-dynamic-s3-environment\nRepository\n:\ndeveloping-dynamic-s3-environment\nPurpose\n: Guides and scripts for creating dynamic S3 environments with MinIO, facilitating efficient storage solutions and scalable deployments.\ncda_POSTS/minio-weaviate\nRepository\n:\nminio-weaviate\nPurpose\n: Integration of MinIO and Weaviate for scalable storage and retrieval, focusing on enhancing AI data workflows and metadata management.\ncda_POSTS/minio-rag-weaviate\nRepository\n:\nminio-rag-weaviate\nPurpose\n: Implementing Retrieval Augmented Generation (RAG) pipelines with MinIO and Weaviate, optimizing AI data processing and retrieval.\ncda_POSTS/minio-aiops-tailscale\nRepository\n:\nminio-aiops-tailscale\nPurpose\n: Combining MinIO, AIOps, and Tailscale to create robust, secure, and automated operational environments.\ncda_POSTS/minio-dns-style\nRepository\n:\nminio-dns-style\nPurpose\n: Implementing DNS-style bucket naming in MinIO, enhancing access control and organization.\ncda_POSTS/minio-docker-sdk\nRepository\n:\nminio-docker-sdk\nPurpose\n: Integrating MinIO with Docker SDK for streamlined development and deployment workflows.\ncda_POSTS/minio-langchain-tool-agent\nRepository\n:\nminio-langchain-tool-agent\nPurpose\n: Developing agents using MinIO and LangChain, facilitating complex data processing and AI integration.\ncda_REPOS\ncda_REPOS/cda-deploy-to-swarm\nRepository\n:\ncda-deploy-to-swarm\nPurpose\n: Central deployment for cda-microservices on Docker Swarm, including configurations for MinIO, Jupyter, MQTT, Nginx, Tailscale, and Weaviate.\ncda_REPOS/cda-namespace-mass-containerization\nRepository\n:\ncda-namespace-mass-containerization\nPurpose\n: Mass containerization within specified namespaces, optimizing the deployment and management of microservices across multiple environments.\ncda_REPOS/cda.hydrate\nRepository\n:\ncda.hydrate\nPurpose\n: Data hydration processes using MinIO and other services, streamlining ETL pipelines.\ncda_REPOS/cda.minio\nRepository\n:\ncda.minio\nPurpose\n: Comprehensive tools and integrations for working with MinIO.\ncda_REPOS/cda.minio-gpt-action\nRepository\n:\ncda.minio-gpt-action\nPurpose\n: GitHub Actions utilizing GPT models for MinIO-related tasks, enhancing automation and data processing capabilities.\ncda_REPOS/cda.minio-system-control\nRepository\n:\ncda.minio-system-control\nPurpose\n: System control tools for MinIO, facilitating efficient management and monitoring.\ncda_REPOS/cda.langchain\nRepository\n:\ncda.langchain\nPurpose\n: Integration of LangChain for advanced AI and data processing workflows with MinIO.\ncda_REPOS/cda.graphql\nRepository\n:\ncda.graphql\nPurpose\n: Provides tools and configurations for setting up and managing GraphQL-based services.\ncda_REPOS/cda.kubernetes\nRepository\n:\ncda.kubernetes\nPurpose\n: Kubernetes configurations and tools for deploying and managing containerized applications.\ncda_REPOS/cda.s3\nRepository\n:\ncda.s3\nPurpose\n: Tools and scripts for managing S3-compatible storage solutions.\ncda_REPOS/cda.rag-weaviate\nRepository\n:\ncda.rag-weaviate\nPurpose\n: Implementing Retrieval Augmented Generation (RAG) pipelines with Weaviate for efficient AI data processing.\ncda_REPOS/cda.vector-db-snapshotter\nRepository\n:\ncda.vector-db-snapshotter\nPurpose\n: Tools for snapshotting and managing vector databases.\ncda_REPOS/cda.weaviate\nRepository\n:\ncda.weaviate\nPurpose\n: Comprehensive tools and configurations for integrating and using Weaviate.\ncda_REPOS/cda.webhooks\nRepository\n:\ncda.webhooks\nPurpose\n: Tools and examples for setting up and managing webhooks for various applications.\ncda_REPOS/cda.s3-engine\nRepository\n:\ncda.s3-engine\nPurpose\n: Engine for managing S3-compatible storage solutions, enhancing data workflows and automation.\ncda_REPOS/cda.s3-api-layer\nRepository\n:\ncda.s3-api-layer\nPurpose\n: API layer for interacting with S3-compatible storage services, providing a robust interface for data operations.\ncda_REPOS/cda.retrieval-plugin\nRepository\n:\ncda.retrieval-plugin\nPurpose\n: Plugin for enhanced data retrieval capabilities, integrating with various data sources.\ncda_REPOS/cda.resumes\nRepository\n:\ncda.resumes\nPurpose\n: Repository for managing and showcasing resumes and professional documents.\ncda_REPOS/cda.prompting\nRepository\n:\ncda.prompting\nPurpose\n: Tools and templates for creating prompts for various AI and automation tasks.\ncda_REPOS/cda.pfsense\nRepository\n:\ncda.pfsense\nPurpose\n: Configuration and management tools for pfSense firewalls.\ncda_REPOS/cda.devices\nRepository\n:\ncda.devices\nPurpose\n: Management and monitoring tools for various devices within the network.\ncda_REPOS/cda.cloud-init.rpi400\nRepository\n:\ncda.cloud-init.rpi400\nPurpose\n: Cloud-init configurations for Raspberry Pi 400, facilitating quick setup and deployment.\ncda_REPOS/cda.jupyterlab\nRepository\n:\ncda.jupyterlab\nPurpose\n: Tools and configurations for setting up and managing JupyterLab environments.\ncda_REPOS/cda.bots\nRepository\n:\ncda.bots\nPurpose\n: Collection of bots for automation.\ncda_REPOS/cda.cloud-init.zima\nRepository\n:\ncda.cloud-init.zima\nPurpose\n: Cloud-init configurations for Zima devices, streamlining deployment and setup processes.\ncda_REPOS/cda.data-lake\nRepository\n:\ncda.data-lake\nPurpose\n: Tools and configurations for setting up and managing data lakes, enhancing data storage and retrieval.\ncda_REPOS/cda.dotfiles\nRepository\n:\ncda.dotfiles\nPurpose\n: Collection of dotfiles for setting up development environments, ensuring consistency and efficiency.\ncda_REPOS/cda.dev-mqqt-broker\nRepository\n:\ncda.dev-mqqt-broker\nPurpose\n: MQTT broker configurations for development environments, enabling efficient message queuing and communication.\ncda_REPOS/cda.mqtt\nRepository\n:\ncda.mqtt\nPurpose\n: Tools and configurations for setting up and managing MQTT services, facilitating IoT and messaging workflows.\ncda_REPOS/cda.MyJunoScripts\nRepository\n:\ncda.MyJunoScripts\nPurpose\n: Collection of scripts for automating tasks within the Juno environment, enhancing productivity and workflow automation.\ncda_REPOS/cda.notebooks\nRepository\n:\ncda.notebooks\nPurpose\n: Jupyter notebooks for data analysis, machine learning, and other computational tasks.\ncda_REPOS/cda.prompting\nRepository\n:\ncda.prompting\nPurpose\n: Tools and templates for creating prompts for various AI and automation tasks.\ncda_REPOS/cda.pfsense\nRepository\n:\ncda.pfsense\nPurpose\n: Configuration and management tools for pfSense firewalls.\ncda_REPOS/cda.devices\nRepository\n:\ncda.devices\nPurpose\n: Management and monitoring tools for various devices within the network.\ncda_REPOS/cda.cloud-init.rpi400\nRepository\n:\ncda.cloud-init.rpi400\nPurpose\n: Cloud-init configurations for Raspberry Pi 400, facilitating quick setup and deployment.\ncda_REPOS/cda.jupyterlab\nRepository\n:\ncda.jupyterlab\nPurpose\n: Tools and configurations for setting up and managing JupyterLab environments.\ncda_REPOS/cda.bots\nRepository\n:\ncda.bots\nPurpose\n: Collection of bots for automation and task management.\ncda_REPOS(cont.)\ncda_REPOS/cda.graphql\nRepository\n:\ncda.graphql\nPurpose\n: Tools and configurations for setting up and managing GraphQL-based services.\ncda_REPOS/cda.kubernetes\nRepository\n:\ncda.kubernetes\nPurpose\n: Kubernetes configurations and tools for deploying and managing containerized applications.\ncda_REPOS/cda.s3\nRepository\n:\ncda.s3\nPurpose\n: Tools and scripts for managing S3-compatible storage solutions.\ncda_REPOS/cda.rag-weaviate\nRepository\n:\ncda.rag-weaviate\nPurpose\n: Implementing Retrieval Augmented Generation (RAG) pipelines with Weaviate for efficient AI data processing.\ncda_REPOS/cda.vector-db-snapshotter\nRepository\n:\ncda.vector-db-snapshotter\nPurpose\n: Tools for snapshotting and managing vector databases.\ncda_REPOS/cda.weaviate\nRepository\n:\ncda.weaviate\nPurpose\n: Comprehensive tools and configurations for integrating and using Weaviate.\ncda_REPOS/cda.webhooks\nRepository\n:\ncda.webhooks\nPurpose\n: Tools and examples for setting up and managing webhooks for various applications.\ncda_REPOS/cda.s3-engine\nRepository\n:\ncda.s3-engine\nPurpose\n: Engine for managing S3-compatible storage solutions, enhancing data workflows and automation.\ncda_REPOS/cda.s3-api-layer\nRepository\n:\ncda.s3-api-layer\nPurpose\n: API layer for interacting with S3-compatible storage services, providing a robust interface for data operations.\ncda_REPOS/cda.retrieval-plugin\nRepository\n:\ncda.retrieval-plugin\nPurpose\n: Plugin for enhanced data retrieval capabilities, integrating with various data sources.\ncda_REPOS/cda.resumes\nRepository\n:\ncda.resumes\nPurpose\n: Repository for managing and showcasing resumes and professional documents.\ncda_REPOS/cda.hydrate\nRepository\n:\ncda.hydrate\nPurpose\n: Data hydration processes using MinIO and other services to streamline ETL pipelines.\ncda_REPOS/cda.minio\nRepository\n:\ncda.minio\nPurpose\n: Comprehensive tools and integrations for working with MinIO, enhancing data management and storage capabilities.\ncda_REPOS/cda.minio-gpt-action\nRepository\n:\ncda.minio-gpt-action\nPurpose\n: GitHub Actions utilizing GPT models for MinIO-related tasks, enhancing automation and data processing capabilities.\ncda_REPOS/cda.minio-system-control\nRepository\n:\ncda.minio-system-control\nPurpose\n: System control tools for MinIO, facilitating efficient management and monitoring.\ncda_REPOS/cda.langchain\nRepository\n:\ncda.langchain\nPurpose\n: Integration of LangChain for advanced AI and data processing workflows with MinIO.\ncda_REPOS/cda.graphql\nRepository\n:\ncda.graphql\nPurpose\n: Tools and configurations for setting up and managing GraphQL-based services.\ncda_REPOS/cda.kubernetes\nRepository\n:\ncda.kubernetes\nPurpose\n: Kubernetes configurations and tools for deploying and managing containerized applications.\ncda_REPOS/cda.s3\nRepository\n:\ncda.s3\nPurpose\n: Tools and scripts for managing S3-compatible storage solutions.\ncda_REPOS/cda.rag-weaviate\nRepository\n:\ncda.rag-weaviate\nPurpose\n: Implementing Retrieval Augmented Generation (RAG) pipelines with Weaviate for efficient AI data processing.\ncda_REPOS/cda.vector-db-snapshotter\nRepository\n:\ncda.vector-db-snapshotter\nPurpose\n: Tools for snapshotting and managing vector databases.\ncda_REPOS/cda.weaviate\nRepository\n:\ncda.weaviate\nPurpose\n: Comprehensive tools and configurations for integrating and using Weaviate.\ncda_REPOS/cda.webhooks\nRepository\n:\ncda.webhooks\nPurpose\n: Tools and examples for setting up and managing webhooks for various applications.\ncda_REPOS/cda.s3-engine\nRepository\n:\ncda.s3-engine\nPurpose\n: Engine for managing S3-compatible storage solutions, enhancing data workflows and automation.\ncda_REPOS/cda.s3-api-layer\nRepository\n:\ncda.s3-api-layer\nPurpose\n: API layer for interacting with S3-compatible storage services, providing a robust interface for data operations.\ncda_REPOS/cda.retrieval-plugin\nRepository\n:\ncda.retrieval-plugin\nPurpose\n: Plugin for enhanced data retrieval capabilities, integrating with various data sources.\ncda_REPOS/cda.resumes\nRepository\n:\ncda.resumes\nPurpose\n: Repository for managing and showcasing resumes and professional documents.\nThank you for exploring my profile! Let's dive into my projects and work together on creating something extraordinary.\nCrafted with dedication and caffeine by David.\nVisitor count\nAbout\nOpen-Source Contributor\nsanity.cdaprod.dev\nTopics\nportfolio\nopensource\nai\nshowcase\nabout-me\nResources\nReadme\nActivity\nStars\n2\nstars\nWatchers\n2\nwatching\nForks\n0\nforks\nReport repository\nReleases\nNo releases published\nSponsor this project\nhttps://www.paypal.me/cdaprod\nhttps://www.buymeacoffee.com/cdasmkt\nLearn more about GitHub Sponsors\nPackages\n0\nNo packages published\nContributors\n2\nLanguages\nHTML\n100.0%\nFooter\n¬© 2024 GitHub,¬†Inc.\nFooter navigation\nTerms\nPrivacy\nSecurity\nStatus\nDocs\nContact\nManage cookies\nDo not share my personal information\nYou can‚Äôt perform that action at this time.\n",
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": "Logs saved to /private/var/mobile/Library/Mobile Documents/iCloud~com~rationalmatter~junoapp/Documents/My Scripts/docker-sdk/minio-fs/scrape_github_cdaprod_cdaprod_c88_logs.txt\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---\n\n# Additional Development Documentation\n\n- Function Serialization with cloudpickle\n- Packaging Python Module\n- Logging with Prometheus Labels\n- Dashboard with Grafana\n- Frontend with Google Mesop\n- Metastore with Python (tbc)\n\n---\n\n# CloudPickle & Docker SDK Method\n\nUsing `cloudpickle` to serialize functions and the Docker SDK to run these functions in isolated environments is a powerful combination for a flexible and dynamic AI orchestration layer. Here‚Äôs how you can integrate `cloudpickle` with Docker SDK to achieve this:\n\n### High-Level Approach\n\n1. **Serialize Functions with `cloudpickle`**: Serialize your Python functions and upload them to MinIO.\n2. **Fetch and Deserialize Functions**: Fetch the serialized functions from MinIO and deserialize them using `cloudpickle`.\n3. **Run Functions in Docker Containers**: Use Docker SDK to create and run containers that execute the deserialized functions.\n\n### Step-by-Step Implementation\n\n#### 1. Serialize and Upload Functions to MinIO\n\n```python\nimport cloudpickle\nfrom minio import Minio\nimport io\n\ndef store_function_in_minio(minio_client, bucket_name, function, function_name):\n    serialized_function = cloudpickle.dumps(function)\n    minio_client.put_object(\n        bucket_name,\n        function_name,\n        data=io.BytesIO(serialized_function),\n        length=len(serialized_function)\n    )\n\n# Initialize MinIO client\nminio_client = Minio(\n    \"play.min.io\",\n    access_key=\"YOUR_ACCESS_KEY\",\n    secret_key=\"YOUR_SECRET_KEY\",\n    secure=True\n)\n\nbucket_name = \"function-bucket\"\nif not minio_client.bucket_exists(bucket_name):\n    minio_client.make_bucket(bucket_name)\n\n# Example function\ndef sample_function(data):\n    return data * 2\n\nstore_function_in_minio(minio_client, bucket_name, sample_function, \"sample_function.pkl\")\n```\n\n#### 2. Fetch and Deserialize Functions\n\n```python\nimport cloudpickle\nfrom minio import Minio\n\ndef fetch_function_from_minio(minio_client, bucket_name, function_name):\n    response = minio_client.get_object(bucket_name, function_name)\n    serialized_function = response.read()\n    return cloudpickle.loads(serialized_function)\n\n# Fetch the function\nsample_function = fetch_function_from_minio(minio_client, bucket_name, \"sample_function.pkl\")\n```\n\n#### 3. Run Functions in Docker Containers\n\nCreate a Docker container that can fetch and execute the serialized functions.\n\n**Dockerfile**:\n\n```Dockerfile\nFROM python:3.9-slim\nWORKDIR /usr/src/app\nCOPY requirements.txt ./\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY script.py ./\nCMD [\"python\", \"./script.py\"]\n```\n\n**requirements.txt**:\n\n```\nminio\ncloudpickle\n```\n\n**script.py**:\n\n```python\nimport os\nimport cloudpickle\nfrom minio import Minio\n\ndef fetch_function_from_minio(minio_client, bucket_name, function_name):\n    response = minio_client.get_object(bucket_name, function_name)\n    serialized_function = response.read()\n    return cloudpickle.loads(serialized_function)\n\nif __name__ == \"__main__\":\n    minio_client = Minio(\n        \"play.min.io\",\n        access_key=os.getenv(\"MINIO_ACCESS_KEY\"),\n        secret_key=os.getenv(\"MINIO_SECRET_KEY\"),\n        secure=True\n    )\n\n    bucket_name = \"function-bucket\"\n    function_name = os.getenv(\"FUNCTION_NAME\")\n\n    # Fetch and execute the function\n    function = fetch_function_from_minio(minio_client, bucket_name, function_name)\n    result = function(5)  # Example input\n    print(result)\n```\n\n#### 4. Orchestration with Docker SDK\n\nUse Docker SDK to build and run the container that fetches and executes the serialized function.\n\n```python\nimport docker\nimport os\n\ndef build_and_run_docker_container(script_path, dockerfile_path='Dockerfile', image_name='function_executor'):\n    client = docker.from_env()\n\n    # Build Docker image\n    image, logs = client.images.build(path='.', tag=image_name)\n    for log in logs:\n        print(log)\n\n    # Run Docker container\n    container = client.containers.run(\n        image_name,\n        environment={\n            \"MINIO_ACCESS_KEY\": \"YOUR_ACCESS_KEY\",\n            \"MINIO_SECRET_KEY\": \"YOUR_SECRET_KEY\",\n            \"FUNCTION_NAME\": \"sample_function.pkl\"\n        },\n        detach=True\n    )\n\n    # Stream logs\n    for log in container.logs(stream=True):\n        print(log.decode('utf-8').strip())\n\n    # Wait for container to finish and get the logs\n    container.wait()\n    container.remove()\n\n# Example usage\nbuild_and_run_docker_container('script.py')\n```\n\n### Conclusion\n\nThis approach combines `cloudpickle` for function serialization, MinIO for object storage, and Docker SDK for isolated execution environments. It allows you to dynamically fetch and run functions in a flexible and scalable way, suitable for building complex AI workflows.\n\nThis setup can be further integrated with LangChain and LCEL for enhanced orchestration and management of AI tasks.\n\n---\n\n# Packaging with Python\n\nCertainly! We can package this entire setup into a Python package. This will allow for easy distribution and installation. Here‚Äôs how you can structure and implement the Python package:\n\n### Directory Structure\n\n```\nai_orchestration/\n‚îú‚îÄ‚îÄ ai_orchestration/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py\n‚îÇ   ‚îú‚îÄ‚îÄ minio_utils.py\n‚îÇ   ‚îú‚îÄ‚îÄ docker_utils.py\n‚îú‚îÄ‚îÄ scripts/\n‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile\n‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt\n‚îÇ   ‚îî‚îÄ‚îÄ script.py\n‚îú‚îÄ‚îÄ tests/\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ test_orchestrator.py\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ setup.py\n‚îî‚îÄ‚îÄ MANIFEST.in\n```\n\n### Implementing the Package\n\n#### 1. `ai_orchestration/__init__.py`\n\nInitialize the package.\n\n```python\n# ai_orchestration/__init__.py\n\nfrom .orchestrator import OrchestrationManager\nfrom .minio_utils import store_function_in_minio, fetch_function_from_minio\nfrom .docker_utils import build_and_run_docker_container\n```\n\n#### 2. `ai_orchestration/orchestrator.py`\n\nThe orchestration manager.\n\n```python\n# ai_orchestration/orchestrator.py\n\nfrom .minio_utils import fetch_function_from_minio\n\nclass OrchestrationManager:\n    def __init__(self, minio_client, bucket_name):\n        self.minio_client = minio_client\n        self.bucket_name = bucket_name\n        self.tasks = []\n\n    def add_task(self, function_name, *args, **kwargs):\n        task = {\"function_name\": function_name, \"args\": args, \"kwargs\": kwargs}\n        self.tasks.append(task)\n\n    def run(self, initial_input):\n        input_data = initial_input\n        for task in self.tasks:\n            function = fetch_function_from_minio(self.minio_client, self.bucket_name, task[\"function_name\"])\n            input_data = function(input_data, *task[\"args\"], **task[\"kwargs\"])\n        return input_data\n```\n\n#### 3. `ai_orchestration/minio_utils.py`\n\nUtilities for MinIO operations.\n\n```python\n# ai_orchestration/minio_utils.py\n\nimport cloudpickle\nimport io\nfrom minio import Minio\n\ndef store_function_in_minio(minio_client, bucket_name, function, function_name):\n    serialized_function = cloudpickle.dumps(function)\n    minio_client.put_object(\n        bucket_name,\n        function_name,\n        data=io.BytesIO(serialized_function),\n        length=len(serialized_function)\n    )\n\ndef fetch_function_from_minio(minio_client, bucket_name, function_name):\n    response = minio_client.get_object(bucket_name, function_name)\n    serialized_function = response.read()\n    return cloudpickle.loads(serialized_function)\n```\n\n#### 4. `ai_orchestration/docker_utils.py`\n\nUtilities for Docker operations.\n\n```python\n# ai_orchestration/docker_utils.py\n\nimport docker\n\ndef build_and_run_docker_container(script_path, dockerfile_path='scripts/Dockerfile', image_name='function_executor'):\n    client = docker.from_env()\n\n    # Build Docker image\n    image, logs = client.images.build(path='scripts', tag=image_name)\n    for log in logs:\n        print(log)\n\n    # Run Docker container\n    container = client.containers.run(\n        image_name,\n        environment={\n            \"MINIO_ACCESS_KEY\": \"YOUR_ACCESS_KEY\",\n            \"MINIO_SECRET_KEY\": \"YOUR_SECRET_KEY\",\n            \"FUNCTION_NAME\": \"sample_function.pkl\"\n        },\n        detach=True\n    )\n\n    # Stream logs\n    for log in container.logs(stream=True):\n        print(log.decode('utf-8').strip())\n\n    # Wait for container to finish and get the logs\n    container.wait()\n    container.remove()\n\n    return container.logs()\n```\n\n#### 5. `scripts/Dockerfile`\n\nThe Dockerfile used to create the execution environment.\n\n```Dockerfile\n# scripts/Dockerfile\n\nFROM python:3.9-slim\nWORKDIR /usr/src/app\nCOPY requirements.txt ./\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY script.py ./\nCMD [\"python\", \"./script.py\"]\n```\n\n#### 6. `scripts/requirements.txt`\n\nThe requirements file for the Docker container.\n\n```\nminio\ncloudpickle\n```\n\n#### 7. `scripts/script.py`\n\nThe script that runs inside the Docker container.\n\n```python\n# scripts/script.py\n\nimport os\nimport cloudpickle\nfrom minio import Minio\n\ndef fetch_function_from_minio(minio_client, bucket_name, function_name):\n    response = minio_client.get_object(bucket_name, function_name)\n    serialized_function = response.read()\n    return cloudpickle.loads(serialized_function)\n\nif __name__ == \"__main__\":\n    minio_client = Minio(\n        \"play.min.io\",\n        access_key=os.getenv(\"MINIO_ACCESS_KEY\"),\n        secret_key=os.getenv(\"MINIO_SECRET_KEY\"),\n        secure=True\n    )\n\n    bucket_name = \"function-bucket\"\n    function_name = os.getenv(\"FUNCTION_NAME\")\n\n    function = fetch_function_from_minio(minio_client, bucket_name, function_name)\n    result = function(5)\n    print(result)\n```\n\n#### 8. `setup.py`\n\nThe setup script for the package.\n\n```python\n# setup.py\n\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='ai_orchestration',\n    version='0.1.0',\n    description='A Python package for AI orchestration using MinIO and Docker',\n    author='Your Name',\n    author_email='your.email@example.com',\n    url='https://github.com/yourusername/ai_orchestration',\n    packages=find_packages(),\n    install_requires=[\n        'minio',\n        'cloudpickle',\n        'docker',\n    ],\n    entry_points={\n        'console_scripts': [\n            'ai_orchestration=ai_orchestration:main',\n        ],\n    },\n)\n```\n\n#### 9. `README.md`\n\nThe readme file for the package.\n\n```markdown\n# AI Orchestration\n\nA Python package for AI orchestration using MinIO and Docker.\n\n## Installation\n\n```bash\npip install ai_orchestration\n```\n\n## Usage\n\n```python\nfrom ai_orchestration import OrchestrationManager, store_function_in_minio\n\n# Initialize MinIO client\nminio_client = Minio(\n    \"play.min.io\",\n    access_key=\"YOUR_ACCESS_KEY\",\n    secret_key=\"YOUR_SECRET_KEY\",\n    secure=True\n)\n\nbucket_name = \"function-bucket\"\nif not minio_client.bucket_exists(bucket_name):\n    minio_client.make_bucket(bucket_name)\n\n# Store a function in MinIO\ndef sample_function(data):\n    return data * 2\n\nstore_function_in_minio(minio_client, bucket_name, sample_function, \"sample_function.pkl\")\n\n# Initialize Orchestration Manager\nmanager = OrchestrationManager(minio_client, bucket_name)\n\n# Add tasks\nmanager.add_task(\"sample_function.pkl\")\n\n# Run the orchestration with initial input\nresult = manager.run({\"data\": 5})\nprint(result)\n```\n```\n\n### Packaging the Python Package\n\nTo package and distribute your Python package, follow these steps:\n\n1. **Build the package**:\n\n```bash\npython setup.py sdist bdist_wheel\n```\n\n2. **Upload to PyPI**:\n\n```bash\npip install twine\ntwine upload dist/*\n```\n\n3. **Install the package**:\n\n```bash\npip install ai_orchestration\n```\n\nThis setup allows you to dynamically fetch, deserialize, and execute functions using MinIO and Docker, all packaged neatly into a Python package. This approach provides a flexible, scalable, and reusable solution for AI orchestration.\n\n---\n\n# With Prometheus Labels\n\nTo use Prometheus for monitoring and incorporate labels for better granularity and organization of your metrics, follow these steps:\n\n### Step-by-Step Implementation\n\n#### 1. Install Prometheus Client\n\nFirst, ensure that the Prometheus client library is installed in your Python environment.\n\n```bash\npip install prometheus_client\n```\n\n#### 2. Set Up Prometheus Metrics in Your Application\n\nCreate a file `metrics.py` in your `ai_orchestration` package to handle Prometheus metrics.\n\n```python\n# ai_orchestration/metrics.py\n\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# Define your metrics\nREQUEST_COUNT = Counter(\n    'request_count', 'Total number of requests', ['endpoint']\n)\nREQUEST_LATENCY = Histogram(\n    'request_latency_seconds', 'Request latency in seconds', ['endpoint']\n)\nACTIVE_USERS = Gauge(\n    'active_users', 'Number of active users'\n)\n\ndef increment_request_count(endpoint: str):\n    REQUEST_COUNT.labels(endpoint=endpoint).inc()\n\ndef observe_request_latency(endpoint: str, latency: float):\n    REQUEST_LATENCY.labels(endpoint=endpoint).observe(latency)\n\ndef set_active_users(count: int):\n    ACTIVE_USERS.set(count)\n```\n\n#### 3. Integrate Prometheus Metrics into Your Orchestration Manager\n\nUpdate your `OrchestrationManager` to include Prometheus metrics.\n\n```python\n# ai_orchestration/orchestrator.py\n\nimport time\nfrom .minio_utils import fetch_function_from_minio\nfrom .metrics import increment_request_count, observe_request_latency\n\nclass OrchestrationManager:\n    def __init__(self, minio_client, bucket_name):\n        self.minio_client = minio_client\n        self.bucket_name = bucket_name\n        self.tasks = []\n\n    def add_task(self, function_name, *args, **kwargs):\n        task = {\"function_name\": function_name, \"args\": args, \"kwargs\": kwargs}\n        self.tasks.append(task)\n\n    def run(self, initial_input):\n        input_data = initial_input\n        for task in self.tasks:\n            start_time = time.time()\n            function = fetch_function_from_minio(self.minio_client, self.bucket_name, task[\"function_name\"])\n            input_data = function(input_data, *task[\"args\"], **task[\"kwargs\"])\n            latency = time.time() - start_time\n            increment_request_count(task[\"function_name\"])\n            observe_request_latency(task[\"function_name\"], latency)\n        return input_data\n```\n\n#### 4. Expose Prometheus Metrics\n\nSet up an endpoint to expose Prometheus metrics. This is typically done in your main application script.\n\n```python\n# main.py or your main application script\n\nfrom flask import Flask, Response\nfrom prometheus_client import generate_latest, CONTENT_TYPE_LATEST\nfrom ai_orchestration import OrchestrationManager, store_function_in_minio\n\napp = Flask(__name__)\n\n@app.route('/metrics')\ndef metrics():\n    return Response(generate_latest(), mimetype=CONTENT_TYPE_LATEST)\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\n```\n\n#### 5. Using Prometheus Labels in Your Workflow\n\nYou can use Prometheus labels to differentiate between different endpoints or tasks in your orchestration. This helps in providing more granular monitoring and analysis of your application‚Äôs performance.\n\n### Example Usage\n\nHere‚Äôs how you can use the updated `OrchestrationManager` with Prometheus metrics.\n\n```python\nfrom minio import Minio\nfrom ai_orchestration import OrchestrationManager, store_function_in_minio\n\n# Initialize MinIO client\nminio_client = Minio(\n    \"play.min.io\",\n    access_key=\"YOUR_ACCESS_KEY\",\n    secret_key=\"YOUR_SECRET_KEY\",\n    secure=True\n)\n\nbucket_name = \"function-bucket\"\nif not minio_client.bucket_exists(bucket_name):\n    minio_client.make_bucket(bucket_name)\n\n# Store a function in MinIO\ndef sample_function(data):\n    return data * 2\n\nstore_function_in_minio(minio_client, bucket_name, sample_function, \"sample_function.pkl\")\n\n# Initialize Orchestration Manager\nmanager = OrchestrationManager(minio_client, bucket_name)\n\n# Add tasks\nmanager.add_task(\"sample_function.pkl\")\n\n# Run the orchestration with initial input\nresult = manager.run({\"data\": 5})\nprint(result)\n```\n\n### Conclusion\n\nIntegrating Prometheus metrics with labels into your AI orchestration layer enhances monitoring capabilities by providing detailed insights into the performance and behavior of different tasks and endpoints. This setup helps in identifying bottlenecks and optimizing the overall workflow.\n\nFor more detailed documentation on Prometheus and its client libraries, refer to:\n- [Prometheus Documentation](https://prometheus.io/docs/introduction/overview/)\n- [Prometheus Python Client Documentation](https://github.com/prometheus/client_python)\n\n---\n\n# With Grafana Dashboard\n\nTo monitor your function store and run functions in the `function-bucket` using Grafana, you need to set up a monitoring stack that includes Prometheus to collect the metrics and Grafana to visualize them. Here‚Äôs a detailed guide to achieve this:\n\n### Step-by-Step Guide\n\n#### 1. Set Up Prometheus\n\nFirst, you need to set up Prometheus to scrape the metrics from your application.\n\n**prometheus.yml**:\n\n```yaml\nglobal:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: 'your_app'\n    static_configs:\n      - targets: ['localhost:5000']  # Assuming your Flask app runs on port 5000\n```\n\nStart Prometheus using Docker:\n\n```bash\ndocker run -d --name prometheus -p 9090:9090 -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus\n```\n\n#### 2. Set Up Grafana\n\nNext, set up Grafana to visualize the metrics collected by Prometheus.\n\nStart Grafana using Docker:\n\n```bash\ndocker run -d --name=grafana -p 3000:3000 grafana/grafana\n```\n\nOpen Grafana in your browser (`http://localhost:3000`), and follow these steps:\n\n1. **Add Data Source**:\n   - Go to Configuration (the gear icon) > Data Sources.\n   - Click \"Add data source\" and select Prometheus.\n   - Set the URL to `http://localhost:9090` and click \"Save & Test\".\n\n2. **Create a Dashboard**:\n   - Go to Create (the plus icon) > Dashboard.\n   - Click \"Add new panel\".\n\n3. **Configure Panels**:\n   - For example, to monitor the number of requests to each endpoint:\n     - **Query**: `sum(rate(request_count[1m])) by (endpoint)`\n     - **Visualization**: Choose a suitable visualization, such as a graph or bar chart.\n   - To monitor request latency:\n     - **Query**: `histogram_quantile(0.95, sum(rate(request_latency_seconds_bucket[5m])) by (le, endpoint))`\n     - **Visualization**: Choose a suitable visualization.\n\n4. **Save the Dashboard**:\n   - Click \"Save dashboard\" (the disk icon), give it a name, and save.\n\n### Example Dashboard Panels\n\nHere are some example panels you can add to your Grafana dashboard:\n\n**1. Total Requests Per Endpoint**:\n\n- **Title**: Total Requests Per Endpoint\n- **Query**: `sum(rate(request_count[1m])) by (endpoint)`\n- **Visualization**: Graph or Bar Chart\n\n**2. Request Latency (95th Percentile)**:\n\n- **Title**: Request Latency (95th Percentile)\n- **Query**: `histogram_quantile(0.95, sum(rate(request_latency_seconds_bucket[5m])) by (le, endpoint))`\n- **Visualization**: Heatmap or Bar Chart\n\n**3. Active Users**:\n\n- **Title**: Active Users\n- **Query**: `active_users`\n- **Visualization**: Gauge or Single Stat\n\n### Monitoring the Function Store\n\nTo monitor specific details about your function store (e.g., number of functions stored, function executions), you can define additional Prometheus metrics and visualize them in Grafana.\n\n**Define Custom Metrics**:\n\n```python\n# ai_orchestration/metrics.py\n\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# Existing metrics\nREQUEST_COUNT = Counter('request_count', 'Total number of requests', ['endpoint'])\nREQUEST_LATENCY = Histogram('request_latency_seconds', 'Request latency in seconds', ['endpoint'])\nACTIVE_USERS = Gauge('active_users', 'Number of active users')\n\n# New metrics for function store\nFUNCTION_STORE_COUNT = Gauge('function_store_count', 'Number of functions in the store')\nFUNCTION_EXECUTIONS = Counter('function_executions', 'Number of function executions', ['function_name'])\n\ndef increment_function_executions(function_name: str):\n    FUNCTION_EXECUTIONS.labels(function_name=function_name).inc()\n\ndef set_function_store_count(count: int):\n    FUNCTION_STORE_COUNT.set(count)\n```\n\n**Update Orchestration Manager**:\n\n```python\n# ai_orchestration/orchestrator.py\n\nfrom .metrics import increment_request_count, observe_request_latency, increment_function_executions, set_function_store_count\n\nclass OrchestrationManager:\n    def __init__(self, minio_client, bucket_name):\n        self.minio_client = minio_client\n        self.bucket_name = bucket_name\n        self.tasks = []\n\n        # Set the initial function count\n        function_count = len(list(self.minio_client.list_objects(bucket_name)))\n        set_function_store_count(function_count)\n\n    def add_task(self, function_name, *args, **kwargs):\n        task = {\"function_name\": function_name, \"args\": args, \"kwargs\": kwargs}\n        self.tasks.append(task)\n\n    def run(self, initial_input):\n        input_data = initial_input\n        for task in self.tasks:\n            start_time = time.time()\n            function = fetch_function_from_minio(self.minio_client, self.bucket_name, task[\"function_name\"])\n            input_data = function(input_data, *task[\"args\"], **task[\"kwargs\"])\n            latency = time.time() - start_time\n            increment_request_count(task[\"function_name\"])\n            observe_request_latency(task[\"function_name\"], latency)\n            increment_function_executions(task[\"function_name\"])\n        return input_data\n```\n\n### Conclusion\n\nBy integrating Prometheus and Grafana, you can effectively monitor your AI orchestration layer, including specific metrics related to your function store. This setup provides detailed insights into your system‚Äôs performance and helps in identifying bottlenecks and optimizing workflows. \n\nFor further customization, refer to the official documentation:\n- [Prometheus Documentation](https://prometheus.io/docs/introduction/overview/)\n- [Grafana Documentation](https://grafana.com/docs/)\n\n---\n\n# Simplify Tooling to Receive Output\n\nTo simplify the input and output processes for your AI tooling, you can design a more user-friendly API or CLI that abstracts the complexity of orchestrating functions, running containers, and handling metrics. Here‚Äôs a step-by-step guide to achieving this:\n\n### Simplified Input/Output with an API\n\n1. **Create a Flask API**: Use Flask to create a simple API that accepts requests and returns results.\n\n**app.py**:\n```python\nfrom flask import Flask, request, jsonify\nfrom minio import Minio\nfrom ai_orchestration import OrchestrationManager, store_function_in_minio\n\napp = Flask(__name__)\n\n# Initialize MinIO client\nminio_client = Minio(\n    \"play.min.io\",\n    access_key=\"YOUR_ACCESS_KEY\",\n    secret_key=\"YOUR_SECRET_KEY\",\n    secure=True\n)\nbucket_name = \"function-bucket\"\nif not minio_client.bucket_exists(bucket_name):\n    minio_client.make_bucket(bucket_name)\n\n# Initialize Orchestration Manager\nmanager = OrchestrationManager(minio_client, bucket_name)\n\n@app.route('/add_task', methods=['POST'])\ndef add_task():\n    data = request.json\n    function_name = data.get('function_name')\n    args = data.get('args', [])\n    kwargs = data.get('kwargs', {})\n    manager.add_task(function_name, *args, **kwargs)\n    return jsonify({\"status\": \"Task added\"}), 200\n\n@app.route('/run', methods=['POST'])\ndef run():\n    initial_input = request.json.get('initial_input', {})\n    result = manager.run(initial_input)\n    return jsonify(result), 200\n\n@app.route('/upload_function', methods=['POST'])\ndef upload_function():\n    file = request.files['file']\n    function_name = file.filename\n    store_function_in_minio(minio_client, bucket_name, cloudpickle.loads(file.read()), function_name)\n    return jsonify({\"status\": \"Function uploaded\"}), 200\n\n@app.route('/metrics')\ndef metrics():\n    return Response(generate_latest(), mimetype=CONTENT_TYPE_LATEST)\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5000)\n```\n\n### Simplified Input/Output with a CLI\n\n1. **Create a CLI Tool**: Use `click` to create a command-line interface for managing tasks and running orchestrations.\n\n**cli.py**:\n```python\nimport click\nimport cloudpickle\nfrom minio import Minio\nfrom ai_orchestration import OrchestrationManager, store_function_in_minio\n\nminio_client = Minio(\n    \"play.min.io\",\n    access_key=\"YOUR_ACCESS_KEY\",\n    secret_key=\"YOUR_SECRET_KEY\",\n    secure=True\n)\nbucket_name = \"function-bucket\"\nif not minio_client.bucket_exists(bucket_name):\n    minio_client.make_bucket(bucket_name)\n\nmanager = OrchestrationManager(minio_client, bucket_name)\n\n@click.group()\ndef cli():\n    pass\n\n@click.command()\n@click.argument('function_name')\n@click.argument('args', nargs=-1)\n@click.option('--kwargs', default={}, type=dict)\ndef add_task(function_name, args, kwargs):\n    manager.add_task(function_name, *args, **kwargs)\n    click.echo(f\"Task {function_name} added.\")\n\n@click.command()\n@click.option('--initial_input', default={}, type=dict)\ndef run(initial_input):\n    result = manager.run(initial_input)\n    click.echo(f\"Result: {result}\")\n\n@click.command()\n@click.argument('file_path')\ndef upload_function(file_path):\n    with open(file_path, 'rb') as file:\n        function = cloudpickle.load(file)\n    function_name = file_path.split('/')[-1]\n    store_function_in_minio(minio_client, bucket_name, function, function_name)\n    click.echo(f\"Function {function_name} uploaded.\")\n\ncli.add_command(add_task)\ncli.add_command(run)\ncli.add_command(upload_function)\n\nif __name__ == \"__main__\":\n    cli()\n```\n\n### Running the CLI Tool\n\nYou can use the CLI to add tasks, run the orchestration, and upload functions:\n\n```bash\npython cli.py add_task sample_function.pkl --args 5\npython cli.py run --initial_input '{\"data\": 5}'\npython cli.py upload_function /path/to/your_function.pkl\n```\n\n### Example Flask API Request\n\nUsing `curl` to interact with the Flask API:\n\n```bash\ncurl -X POST http://localhost:5000/add_task -H \"Content-Type: application/json\" -d '{\"function_name\": \"sample_function.pkl\", \"args\": [5], \"kwargs\": {}}'\ncurl -X POST http://localhost:5000/run -H \"Content-Type: application/json\" -d '{\"initial_input\": {\"data\": 5}}'\ncurl -F 'file=@/path/to/your_function.pkl' http://localhost:5000/upload_function\n```\n\n### Conclusion\n\nBy creating a simplified API or CLI, you abstract the complexities of managing tasks, running orchestrations, and handling metrics. This makes it easier for users to interact with your AI tooling without needing to understand the underlying implementation details. This approach improves usability and allows for easy integration into various workflows and systems.\n\n---\n\n# Google's Mesop (Frontend with Python)\n\n**Integrating Your AI Tooling with Google Mesop**\n\nTo interface your AI orchestration layer with a Google Mesop chatbot, you can leverage Mesop's capabilities to build a UI and handle user interactions seamlessly. Mesop is a Python-based UI framework developed by Google for quickly building web applications using idiomatic Python code. This framework can help you create interactive interfaces without diving into JavaScript, CSS, or HTML.\n\n### Key Features of Mesop\n\n1. **Component-Based Architecture**: Mesop allows you to build UIs using reusable components, which are essentially Python functions. This makes the UI development process straightforward and modular.\n2. **Hot Reload**: It supports hot reloading, which means changes in your code will reflect immediately in the browser without restarting the server.\n3. **Integration with Python Ecosystem**: Mesop can integrate well with other Python libraries and frameworks, making it a good choice for extending existing Python projects [oai_citation:1,Quickstart - Mesop](https://google.github.io/mesop/getting_started/quickstart/) [oai_citation:2,GitHub - google/mesop: Build delightful web apps quickly in Python](https://github.com/google/mesop) [oai_citation:3,Mesop](https://google.github.io/mesop/).\n\n### Setting Up Mesop\n\nFirst, you need to install Mesop. Follow these steps:\n\n1. **Install Mesop**:\n\n   ```bash\n   pip install mesop\n   ```\n\n2. **Create a Simple Mesop Application**:\n\n   Create a `main.py` file:\n\n   ```python\n   import mesop as me\n   import mesop.labs as mel\n\n   @me.page(path=\"/\")\n   def app():\n       me.text(\"Hello, this is your AI orchestration interface!\")\n       if me.button(\"Run Orchestration\"):\n           result = run_orchestration()\n           me.text(f\"Result: {result}\")\n\n   def run_orchestration():\n       # Placeholder for orchestration logic\n       return \"Orchestration run successfully!\"\n\n   if __name__ == \"__main__\":\n       me.run()\n   ```\n\n3. **Run the Application**:\n\n   ```bash\n   mesop main.py\n   ```\n\n   Navigate to `http://localhost:8000` to see your application.\n\n### Integrating AI Tooling with Mesop\n\nYou can expand this basic setup to include the functionality of your AI orchestration layer. Here‚Äôs how you can integrate the orchestration tasks and function executions:\n\n1. **Expand the Orchestration Logic**:\n\n   Update `run_orchestration` to interact with your AI orchestration layer.\n\n   ```python\n   from minio import Minio\n   from ai_orchestration import OrchestrationManager, store_function_in_minio\n   import cloudpickle\n\n   # Initialize MinIO client\n   minio_client = Minio(\n       \"play.min.io\",\n       access_key=\"YOUR_ACCESS_KEY\",\n       secret_key=\"YOUR_SECRET_KEY\",\n       secure=True\n   )\n   bucket_name = \"function-bucket\"\n   if not minio_client.bucket_exists(bucket_name):\n       minio_client.make_bucket(bucket_name)\n\n   manager = OrchestrationManager(minio_client, bucket_name)\n\n   @me.page(path=\"/\")\n   def app():\n       me.text(\"Hello, this is your AI orchestration interface!\")\n       if me.button(\"Run Orchestration\"):\n           result = run_orchestration()\n           me.text(f\"Result: {result}\")\n\n   def run_orchestration():\n       manager.add_task(\"sample_function.pkl\")\n       result = manager.run({\"data\": 5})\n       return result\n\n   if __name__ == \"__main__\":\n       me.run()\n   ```\n\n2. **Add Task and Run Functions via the UI**:\n\n   Extend the Mesop UI to allow uploading functions and adding tasks dynamically.\n\n   ```python\n   @me.page(path=\"/upload\")\n   def upload_function():\n       me.text(\"Upload a new function\")\n       function_file = me.file_input(label=\"Choose a function file\")\n       if me.button(\"Upload\") and function_file:\n           function_data = function_file.read()\n           function_name = function_file.filename\n           store_function_in_minio(minio_client, bucket_name, cloudpickle.loads(function_data), function_name)\n           me.text(f\"Function {function_name} uploaded successfully.\")\n\n   @me.page(path=\"/add_task\")\n   def add_task():\n       me.text(\"Add a new task\")\n       function_name = me.text_input(label=\"Function name\")\n       args = me.text_input(label=\"Arguments (comma-separated)\")\n       if me.button(\"Add Task\"):\n           args_list = args.split(',')\n           manager.add_task(function_name, *args_list)\n           me.text(f\"Task {function_name} added with args {args_list}.\")\n\n   @me.page(path=\"/\")\n   def main_app():\n       me.text(\"AI Orchestration Dashboard\")\n       me.link(\"Upload Function\", path=\"/upload\")\n       me.link(\"Add Task\", path=\"/add_task\")\n       if me.button(\"Run Orchestration\"):\n           result = run_orchestration()\n           me.text(f\"Result: {result}\")\n\n   if __name__ == \"__main__\":\n       me.run()\n   ```\n\n### Conclusion\n\nBy integrating Mesop with your AI orchestration layer, you create a user-friendly interface that allows users to upload functions, add tasks, and run orchestrations directly from a web application. Mesop‚Äôs simplicity and Python-first approach make it an excellent choice for extending your existing Python projects with interactive UIs.\n\nFor more details on Mesop and its capabilities, you can refer to the [Mesop documentation](https://google.github.io/mesop/) [oai_citation:4,Mesop](https://google.github.io/mesop/) [oai_citation:5,Why Mesop? - Mesop](https://google.github.io/mesop/blog/2024/05/13/why-mesop/).\n\n---\n\n# Syncing MinIO Objectstore & Weaviate Metastore \n\nTo synchronize the MinIO bucket and the Weaviate \"Feature\" schema class as we built previously with Golang, we need to ensure that the Python implementation correctly handles both the upload to MinIO and the metadata synchronization with Weaviate. Here‚Äôs how to extend the existing Python implementation to achieve this:\n\n### Synchronize MinIO and Weaviate in Python\n\nBelow is the updated code to include synchronization with the Weaviate \"Feature\" schema class:\n\n#### Dependencies\nMake sure to install the required dependencies:\n```bash\npip install minio weaviate-client docker\n```\n\n#### Initializer Class with Weaviate Synchronization\n\n```python\n# feature_store_initializer.py\n\nimport os\nimport docker\nimport cloudpickle\nfrom minio import Minio\nfrom minio.error import S3Error\nfrom weaviate import Client as WeaviateClient, AuthClientPassword\n\nclass FeatureStoreInitializer:\n    def __init__(self, minio_config, weaviate_config, docker_config):\n        self.minio_client = Minio(**minio_config)\n        self.weaviate_client = WeaviateClient(**weaviate_config)\n        self.docker_client = docker.DockerClient(**docker_config)\n        self.bucket_name = \"function-bucket\"\n    \n    def create_bucket(self):\n        try:\n            if not self.minio_client.bucket_exists(self.bucket_name):\n                self.minio_client.make_bucket(self.bucket_name, location=\"us-east-1\")\n                print(f\"Bucket {self.bucket_name} created successfully.\")\n            else:\n                print(f\"Bucket {self.bucket_name} already exists.\")\n        except S3Error as exc:\n            print(f\"Error occurred: {exc}\")\n    \n    def upload_script(self, script_name, script_content):\n        try:\n            self.minio_client.put_object(self.bucket_name, script_name, script_content, len(script_content))\n            print(f\"Script {script_name} uploaded successfully.\")\n        except S3Error as exc:\n            print(f\"Error occurred: {exc}\")\n\n    def fetch_script(self, object_name, file_path):\n        try:\n            self.minio_client.fget_object(self.bucket_name, object_name, file_path)\n            print(f\"Script {object_name} fetched and saved to {file_path}\")\n        except S3Error as exc:\n            print(f\"Error occurred: {exc}\")\n\n    def store_function_in_minio(self, function, function_name):\n        serialized_function = cloudpickle.dumps(function)\n        self.upload_script(function_name, serialized_function)\n        # Synchronize with Weaviate\n        self.sync_with_weaviate(function_name)\n\n    def sync_with_weaviate(self, function_name):\n        properties = {\n            \"name\": function_name,\n            \"description\": \"A serialized function\",\n            \"libraries\": \"cloudpickle\",\n            \"input_source\": \"N/A\",\n            \"output_source\": \"N/A\",\n            \"category\": \"function\",\n            \"docker_image\": \"N/A\",\n            \"object_path\": f\"https://{self.minio_client._endpoint}/{self.bucket_name}/{function_name}\",\n            \"version\": \"1.0\"\n        }\n        self.weaviate_client.data_object.create(properties, \"Feature\")\n        print(f\"Synchronized {function_name} with Weaviate\")\n\n    def build_docker_image(self, script_path, dockerfile_path='Dockerfile', image_name='script_executor'):\n        if not os.path.exists(dockerfile_path):\n            dockerfile_content = f\"\"\"\nFROM python:3.9-slim\nWORKDIR /usr/src/app\nCOPY {os.path.basename(script_path)} /usr/src/app/{os.path.basename(script_path)}\nRUN pip install requests beautifulsoup4\nRUN chmod +x /usr/src/app/{os.path.basename(script_path)}\nCMD [\"python\", \"/usr/src/app/{os.path.basename(script_path)}\"]\n\"\"\"\n            with open(dockerfile_path, 'w') as dockerfile:\n                dockerfile.write(dockerfile_content.strip())\n            print(f\"Generated default Dockerfile at {dockerfile_path}\")\n        else:\n            print(f\"Using existing Dockerfile at {dockerfile_path}\")\n\n        image, logs = self.docker_client.images.build(path='.', tag=image_name, dockerfile=dockerfile_path, nocache=True)\n        for log in logs:\n            print(log)\n        return image\n\n    def run_docker_container(self, image_name, environment):\n        container_name = f'script_container_{uuid.uuid4()}'\n        container = self.docker_client.containers.run(image_name, name=container_name, detach=True, environment=environment)\n        \n        logs = container.logs(stream=True)\n        \n        for log in logs:\n            print(log.decode('utf-8').strip())\n        \n        container.wait()\n        container.remove()\n        print(f\"Container {container_name} finished execution.\")\n\n    def initialize(self, function_name, environment):\n        self.create_bucket()\n        self.store_function_in_minio(self.sample_function, function_name)\n        script_path = os.path.join(os.getcwd(), function_name)\n        self.fetch_script(function_name, script_path)\n        self.build_docker_image(script_path)\n        self.run_docker_container('script_executor', environment)\n    \n    def sample_function(self, data):\n        return data * 2\n\ndef scrape_url(url):\n    minio_config = {\n        \"endpoint\": \"192.168.0.25:9000\",\n        \"access_key\": \"cda_cdaprod\",\n        \"secret_key\": \"cda_cdaprod\",\n        \"secure\": False\n    }\n\n    weaviate_config = {\n        \"url\": \"http://localhost:8080\",\n        \"auth_client_secret\": AuthClientPassword(username=\"user\", password=\"password\")\n    }\n\n    docker_config = {\n        \"base_url\": 'tcp://rpi4-2.local:2375'\n    }\n\n    initializer = FeatureStoreInitializer(minio_config, weaviate_config, docker_config)\n    initializer.initialize(\"sample_function.pkl\", {\"URL\": url})\n\n# Example usage within the notebook\nscrape_url(\"https://github.com/cdaprod/cdaprod\")  # Replace this with the actual URL you want to scrape\n```\n\n### Explanation:\n\n1. **FeatureStoreInitializer Class**:\n   - **Initialization**: Sets up MinIO, Weaviate, and Docker clients.\n   - **Bucket Management**: Creates and checks for the existence of a MinIO bucket.\n   - **Script Management**: Uploads and fetches scripts to/from MinIO.\n   - **Function Serialization**: Serializes functions using `cloudpickle` and stores them in MinIO.\n   - **Weaviate Synchronization**: Synchronizes the uploaded function with the Weaviate \"Feature\" schema class by creating a corresponding object in Weaviate with the required properties.\n   - **Docker Image Management**: Builds Docker images from provided scripts.\n   - **Container Execution**: Runs Docker containers and logs their outputs.\n\n2. **scrape_url Function**:\n   - Configures MinIO, Weaviate, and Docker settings.\n   - Initializes the FeatureStoreInitializer and runs a sample function.\n\n### Additional Considerations\n\n- **Weaviate Schema and Data Management**: Define the schema and manage metadata in Weaviate, similar to the previous example but with integration into this Python class.\n- **Testing and Validation**: Ensure to test the integration and handle any potential errors gracefully.\n\nThis setup simplifies the interaction between MinIO and Weaviate while maintaining essential functionalities and compatibility with your current system."
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}